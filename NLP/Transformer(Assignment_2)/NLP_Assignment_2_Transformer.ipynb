{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtrTdPCkY8tS"
      },
      "source": [
        "# 6 - Attention is All You Need\n",
        "\n",
        "V2022117 황민규\n",
        "\n",
        "## Introduction\n",
        "\n",
        "\n",
        "NLP 분야에서에서는 Recurrent Neural Network(RNN)기반을 기반으로 하는 seq2seq, 또는 RNN + Attention기반의 딥러닝 모델을 대부분 사용하였다. 하지만, RNN 모델의 경우 상당한 계산 복잡도를 가지며, 순차적으로 연산을 하기 때문에 병렬처리가 불가능하다는 점과 단어간의 사이가 멀어질수록 참조가 잘 되지 않는 long-range dependency 문제가 있다. 따라서 이 문제를 해결하기 위해 나온 모델이 바로 transformer이다. 해당 논문에서는 앞서 발생한 문제점들을 해결하기 위해 Self-attention Mechanism을 제시하였으며, RNN이나 LSTM 모델 없이 attention mechanism만을 사용하여 machine translation task를 진행하였고, 매우 좋은 성능을 낸 것을 확인할 수 있다. 그러므로 논문 제목부터 Attention is All you need인 것이다.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUw1TY_6a6Q-",
        "outputId": "7134aad5-37c3-4b38-85df-a911c664927a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.11.0\n",
            "  Downloading torchtext-0.11.0-cp38-cp38-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.0) (4.64.1)\n",
            "Collecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x3a7fc000 @  0x7effc5bb3615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 4.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.11.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.0\n",
            "    Uninstalling torchtext-0.14.0:\n",
            "      Successfully uninstalled torchtext-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0 torchtext-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from de-core-news-sm==3.4.0) (3.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "#torchtext.legacy 사용을 위해 torchtext를 이전버전으로 받아줄 필요가 있다, 여기서는 0.9.0버전으로 진행하였다.\n",
        "!pip install torchtext==0.11.0\n",
        "#영어, 독일어 spacy 모델을 다운로드\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT6SFqG-Y8tU"
      },
      "source": [
        "## Preparing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lce1xXqPY8tV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nGLDIhl_jbN"
      },
      "source": [
        "난수 생성기의 seed를 고정하면, 매번 프로그램을 실행할 때 마다 생성되는 난수들의 수열이 같게 할 수 있다. 이를 통해 반복 실행시, 결과가 재현되도록 해주었다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zvnpbRiYY8tW"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alSEj0BHY8tW"
      },
      "source": [
        "spacy는 파이썬의 자연어처리를 위한 오픈 소스 기반 라이브러리로 Tokeniztion(입력 텍스트를 단어, 문장 부호 등의 토큰으로 분류하는 과정), POS Tagging(문장 내 단어들의 품사를 식벼라여 태그를 붙여주는 과정), Dependency Parsing(각 token 들간의 의존관계를 고려하여 관련있는 단어들끼리 묶는 과정) 등의 기능을 제공한다. \n",
        "\n",
        "독일어를 위한 spaCy 모델은 \"de_core_news_sm\"이고, 영어를 위한 모델은 \"en_core_web_sm\"이며 각 모델의 tokenizer에 접근하기 위해 해당 모델을 로드하였다. \n",
        "tokenizer는 문장이 인풋으로 들어왔을때 각 단어에 대해 정해진 token으로 분류해주는 기능을 한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6_G4dWIhY8tW"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ltQYVmZ0Y8tX"
      },
      "outputs": [],
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdBvPEKpY8tX"
      },
      "source": [
        "torchtext 에는 필드(field)라는 도구를 제공한다. 필드를 통해 앞으로 어떤 전처리를 할 것인지를 정의한다. \n",
        "\n",
        "SRC field 안에서는 tokenizer로 tokenize_de 함수(독일어 tokenizer)를 사용하며,init_token으로는 sos(start of sequence)\n",
        "eos_token(end token)으로는 eos(end of sequece) 토큰을 추가한다.\n",
        "\n",
        "`lower = true`는 들어오는 모든 스팰링을 소문자로 변환하는 기능이다.\n",
        "\n",
        "`batch_first = True` 를 이용하여 미니 batch의 차원을 맨 앞으로 하여 데이터를 불러온다. \n",
        "\n",
        " TPG field 안에서는 tokenizer로 tokenize_en 함수(영어 tokenizer)를 사용하며, 나머지는 SRC field(독일어)와 동일하다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t4xS1wJtY8tX"
      },
      "outputs": [],
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUewjueCY8tY"
      },
      "source": [
        "dataset으로는 Multi30k dataset을 사용하였다. 이는 약 30,000 개의 영어, 독일어, 프랑스어로 이루어진 문장들이 포함되어 있는 데이터이며, 한 문장당 대략 12개의 단어로 구성되어 있다. \n",
        "\n",
        "exts는 source와 target으로 사용될 언어를 지정하며(source가 첫번째), fields는 source와 target에서 사용될 전처리 도구인 field를 지정해준다. \n",
        "splits 함수를 통해 Multi30k dataset을 분할하여 train, validation, test data를 얻는다.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG_x2jOLY8tY",
        "outputId": "c7edec14-9d55-438a-9b21-9e410137f077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:00<00:00, 5.55MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46.3k/46.3k [00:00<00:00, 1.02MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 66.2k/66.2k [00:00<00:00, 1.48MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "                                                    fields = (SRC, TRG))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzeYGh2mA3Jj",
        "outputId": "8e989986-5f6e-4ce6-ebc8-1bb8a811a4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋(train_data) 크기: 29000개\n",
            "평가 데이터셋(valid_data) 크기: 1014개\n",
            "테스트 데이터셋(test_data) 크기: 1000개\n"
          ]
        }
      ],
      "source": [
        "print(f\"학습 데이터셋(train_data) 크기: {len(train_data.examples)}개\")\n",
        "print(f\"평가 데이터셋(valid_data) 크기: {len(valid_data.examples)}개\")\n",
        "print(f\"테스트 데이터셋(test_data) 크기: {len(test_data.examples)}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4_UXHCTBMTW"
      },
      "source": [
        "train_data, valid_data, test_data의 갯수를 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbHU4vQKCrRU"
      },
      "source": [
        "아래의 코드는 *vocabulary*를 생성한다. vocabulary는 각각의 고유한 토큰을 indexing해주는 데 사용된다. source 와 target 언어의 vocabulary는 서로 구분된다. \n",
        "\n",
        "min_freq 인수를 사용하여 단어가 출현한 횟수가 2보다 큰 단어들만 vocabulary에 추가해준다. 한번만 등장하는 단어들은 unknown token <unk>로 처리된다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5A20tsUUY8tZ"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLvvA23xY8tZ"
      },
      "source": [
        "선언한 device는 GPU로 설정이 되었으며, GPU를 사용할 수 없는 경우 CPU를 사용한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S7VuF18BY8tZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFXUOutlBxtY",
        "outputId": "b1e4469f-bde6-49ea-abc0-a3877e258dda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Asy4NEB1Po"
      },
      "source": [
        "위의 코드로 현재 device는 cuda라는것을 확인할 수 있다.\n",
        "즉 현재 GPU를 사용하고 있다는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG5W7JbdEZJR"
      },
      "source": [
        "BucketIterator를 생성한다. 이는 Pytorch의 dataloader와 쓰임새가 같다. 즉, 배치 크기 단위로 값을 차례대로 꺼내어 메모리로 가져오고 싶을 때 사용한다. \n",
        "\n",
        "하지만 dataloader와의 차이점은 BucketIterator는 비슷한 길이의 문장들끼리 batch를 만들기 때문에 padding의 개수를 최소화할 수 있다는 장점이 있다.\n",
        "\n",
        "\n",
        "자연어 처리를 하다보면 필연적으로 각 문장의 길이(단어 수)가 다른 경우가 발생한다, 이때 데이터에 특정값을 채워서 각 문장의 길이를 맞추어주는 과정을 거치는데, 이 과정을 padding 이라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-FlDl-mYY8ta"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu2w84w1Y8ta"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "\n",
        "\n",
        "### Encoder\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1xFY16_EI0e88JNxRuszOQTg1Ida035md\" height = 300 width = 300>\n",
        "\n",
        "Encoder 부분을 살펴보면, Encoder block이 N개(논문에서는 6개) 사용하였다. \n",
        "위 그림을 보면, encoder block의 내부는 2개의 sub-layer로 이루어져 있으며, 각각 Multi-Head Attention, Position-wise Feed-Forward Networks이다.\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePfsh7FzjRf7"
      },
      "source": [
        "### positional embedding \n",
        "\n",
        "첫번째 encoder block에 들어가는 입력값은 각각의 token들이 갖고 있는 임베딩 벡터만에 추가적으로 positional embedding 벡터 정보를 더해서 그 결과를 최종적인 임베딩 벡터로 사용을 하였다.\n",
        "\n",
        "문장에는 단어간 순서가 있다, RNN과 같은 모델은 그러한 순서를 구분할 수 있지만, 해당 모델은 모든 단어를 한번에 인풋으로 받기때문데 위치 정보를 알 수가 없다. 그렇기에 positional embedding을 통해 문장에서 단어간 위치 정보를 따로 부여해줄 필요가 있다.\n",
        "\n",
        "positional embedding vector의 원소의 값을 계산하는 식은 아래와 같다.\n",
        "\n",
        "$$ \\begin{matrix}\n",
        "PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{\\text{model}}})\\\\\n",
        "PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{\\text{model}}})\n",
        "\\end{matrix} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245Wa009FcNE"
      },
      "source": [
        "##Encoder\n",
        "\n",
        "* 전체 인코더 아키텍처를 정의한다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout**: 드롭아웃(dropout) 여부\n",
        "    * **device**: 어떤 디바이스(CPU or GPU)인지\n",
        "    * **max_length**: 문장 내 최대 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8xXNAcivY8ta"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        '''\n",
        "        <sos> token부터 문장의 최대 길이로 설정한 max_length =100까지 positional embedding 을 한다.  \n",
        "        '''\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "       \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        #dropout은 일부 파라미터를 학습에 반영하지 않음으로써 모델을 일반화하는 방법이다.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #self.scale은 key 벡터의 차원에 root를 취한 값의 역할을 한다.\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        #문장의 embedding vector + positional embedding vector  \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #마지막 레이어의 출력을 반환한다.     \n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwvWhnJCY8tb"
      },
      "source": [
        "### Encoder Layer\n",
        "\n",
        "앞서 소개한 두개의 sub-layer 외에도 Add & Norm layer를 추가하여 성능을 높였다. \n",
        "\n",
        "Add : 이 부분에서는 self-attention의 출력값과 입력값을 더해주는 역할을 한다.\n",
        "\n",
        "Layer normalization : FeedForward NN 에서 발생할 수 있는 과적합 문제를 해결하기 위해서 사용하는 normalize 방법이며, 각 input의 feature들에 대한 평균과 분산을 구해서 batch에 있는 각 input을 normalization 하는 방법이다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout**: 드롭아웃(dropout) 여부\n",
        "    * **device**: 어떤 디바이스(CPU or GPU)인지\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r0ZExTz7Y8tb"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "\n",
        "        #dropout, residual connection and layer norm\n",
        "        '''\n",
        "        이 과정은 Add&Norm layer에 해당한다. \n",
        "        '''\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        # Add&Norm layer의 결과값으로 얻은 src는 두개의 FC(fully connected) layer로 이루어진 position-wise feedforward layer를 통과한다. \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #마지막 레이어의 출력을 반환한다. \n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep4Odv2-Y8tb"
      },
      "source": [
        "### Mutli Head Attention Layer\n",
        "\n",
        "아래의 코드는 앞서 설명한 encoder block의 두개의 sub-layer중 multi head attention layer에 해당한다. \n",
        "<img src = \"https://drive.google.com/uc?id=1q0Cf4YkU42sa-5afNsMDTGA1JPTx0Dhv\" height = 300 width = 300>\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1Mu-0is9QALm2TCOTRoLncFZaGEAHtTeE\" height = 300 width = 300>\n",
        "\n",
        "*Multi-Head*는 self-attention을 여러개 적용했다는 것을 의미한다.\n",
        "h개의 Scaled Dot-Product attention 연산을 수행하며 진행한다.\n",
        "어텐션은 세 가지 요소를 입력으로 받으며 각각 query, key, value이다. \n",
        "본 코드에서는 linear projection을 하기 전의 query, key, value 벡터의 차원은 ${d_{model} = 512}$이며, 마지막 linear projection 을 거친 후의 query, key, value 벡터의 차원은  $ d_k=d_v=d_{\\text{model}}/h = 64 $ 가 된다. \n",
        "\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **dropout**: 드롭아웃(dropout) 여부\n",
        "    * **device**: 어떤 디바이스(CPU or GPU)인지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Lgqv48rZY8tc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "   \n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim #하나의 단어에 대한 임베딩 차원 \n",
        "        self.n_heads = n_heads #헤드(head)의 개수 : 서로 다른 scaled dot-product attention의 개수\n",
        "        self.head_dim = hid_dim // n_heads #각 헤드(head)에서의 임베딩 차원 -> 각 헤드의 차원 * 헤드의 개수 = hid_dim(임베딩 차원) 이어야 함\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim) # Query 값에 적용될 FC 레이어 \n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim) # Key 값에 적용될 FC 레이어 \n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #각각의 query,key,value의 차원에 해당하는 self.head_dim에 root를 씌운 값을 나누어서 softmax 함수에 넣어줄 수 있도록 만든 것\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim] 이때 query len은 단어의 개수에 해당한다. \n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "\n",
        "        #query, key, value를 각각 Q,K,V에 맵핑을 해준다.        \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "        '''\n",
        "        Q,K,V를 각각 n_heads개로 나누어 사용하기 위해, 각각을 hidden_dim -> n_heads * head_dim 의 형태로 변형해준다.\n",
        "        즉, n_heads 개 각각마다 head_dim 의 차원을 가지도록 만들어 준다. 이는 각각 n_heads 개의 query, key, value를 만들어준것과 같다. \n",
        "\n",
        "        '''\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "        '''\n",
        "        Attention energy 즉, attention score를 계산하는 코드이다. \n",
        "        각각의 head 마다 Q(query)와 K(key)를 서로 곱해준 후, 앞서 구한 self.scale로 나누어 주어서 energy(attention score)를 구한다.         \n",
        "        '''\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        '''\n",
        "        필요하다면 마스크를 씌울 수 있다. \n",
        "        마스크(Mask)를 사용하는 경우에는, 마스크 값이 0인 부분을 (-무한대)에 가까운 값인 -1e10으로 채워준다.  \n",
        "        따라서 softmax 함수를 적용하였을 때의 결과값이 0이 될 수 있도록 만들어준다. \n",
        "        '''\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        '''\n",
        "        energy에 softmax 함수를 적용하여 attention 가중치 값을 구함\n",
        "        attention 가중치 값은 각 단어에 대한 확률 값에 해당한다. \n",
        "        '''\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        #앞에서 얻은 attention 가중치 값들을 V(value)와 곱하면 최종적으로 우리가 관심있어하는 단어에 대한 self attention의 결과를 얻을 수 있다.        \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        '''\n",
        "        view를 이용하여 x를 다시 일자로 쭉 늘어뜨려서, concat을 수행한 것과 동일한 결과를 얻을 수 있다. \n",
        "        '''\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        #마지막으로 output linear 함수를 거친 후, 결과 x와 attention 가중치를 정의하고 있는 함수의 결과로 출력한다. \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wlWgzCY8tc"
      },
      "source": [
        "### Position-wise Feedforward Layer\n",
        "\n",
        "아래의 코드는 앞서 설명한 encoder block의 두개의 sub-layer중 Position-wise Feedforward layer에 해당한다. 해당 논문에서 input과 output의 dimension은 ${d_{model} = 512}$이며, inner-layer의 dimension은 ${d_{ff} = 2048}$이다. 일반적으로 inner-layer의 dimension은 input과 output의 dimension보다 훨씬 크게 설정하며, 첫번째 FC layer과 두번째 FC layer 사이에서 Relu 활성화 함수와 dropout을 적용한다. \n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout**: 드롭아웃(dropout) 여부"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0dUgcZDRY8tc"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        '''\n",
        "        fc_1 layer의 입력으로 hid_dim 만큼의 차원이 입력되었을 때, \n",
        "        fc_2 layer의 출력으로 hid_dim 만큼의 차원을 그대로 내보낸다. \n",
        "        즉 입력과 출력의 차원이 동일하다. \n",
        "        '''\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)  \n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        #활성화 함수 ReLu는 fc_1 layer에만 적용된다. \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Os_t_YY8tc"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1nypaHsZJDnZShwK7CJx5-X9F7f09JQzX\" height = 600 width = 300>\n",
        "\n",
        "Transformer의 Decoder 부분은 seq2seq의 decoder와 비슷한 역할을 한다. \n",
        "\n",
        "Decoder block의 전체적인 구조는 앞에서 설명한 encoder block과 유사하다, 하지만 decoder에서는 masked multi-head attention이 사용된다는 차이점이 있다. \n",
        "\n",
        "masked multi-head attention을 사용함으로써, decoder에서는 앞에 들어온 단어만 볼 수 있으며, 뒤의 단어는 확인이 불가하며, 뒤의 단어를 예측하는 역할을 한다.\n",
        "\n",
        "디코더에서 사용되는 또다른 attention은 *multi head attention(encoder attention)*으로 self-attention 과 마찬가지로 query, key, value 벡터들을 사용한다. query 벡터는 디코더 부분에 입력된 단어에 대한 query 벡터를 사용한다. 때문에 디코더 부분에 입력된 단어의 query 벡터를 계산하기 위해서 해당 attention layer 아래에 있는 Add & Norm 층이 출력하는 결과를 이용한다. key와 value 벡터는 인코더 부분에 입력된 각 단어에 대한 key, value 값을 사용한다. 작동하는 방식은 self-attention과 동일하다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Y0MAqQi_Y8tc"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        '''\n",
        "         nn.Embedding()은 룩업 테이블 과정을 수행한다. 즉, 특정 토큰과 맵핑되는 정수(이는 vocabulary에서 지정됨)를 인덱스로 가지는 \n",
        "        테이블(임베딩 테이블)로부터 임베딩 벡터 값을 가져온다. nn.Embedding()은 크게 두 가지 인자를 받는데, 각각 output_dim과 hid_dim 이다. \n",
        "        output_dim은 임베딩을 할 단어들의 개수로, 다시 말해 단어 집합의 크기이며, 임베딩 테이블은 인자로 받은 단어 집합의 크기만큼 행을 가진다. \n",
        "        hid_dim은 특정 토큰과 맵핑되는 정수를 인덱스로 가지는 벡터(즉, 임베딩 벡터)의 차원에 해당하며 , 이는 사용자가 정해주는 하이퍼파라미터이다. \n",
        "        '''\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim) \n",
        "        '''\n",
        "        <sos> token부터 문장의 최대 길이로 설정한 max_length =100까지 positional embedding 을 한다. \n",
        "        '''\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        '''\n",
        "        nn.ModuleList 함수를 통해 DecoderLayer 모듈을 리스트 형태로 저장하였다. n_layers의 개수만큼 DecoderLayer 모듈이 list에 저장이 된다. \n",
        "        해당 논문에서는 총 6개의 DecoderLayer 모듈을 사용했지만, 해당 코드에서는 총 3개의 DecoderLayer 모듈을 사용하였다. \n",
        "        nn.ModuleList를 이용하면 파이썬의 일반적인 리스트처럼 인덱스로 접근이 가능하다.   \n",
        "        '''\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len] -> teacher forcing을 하기 위한 정답 sequence data에 해당한다.  \n",
        "        #enc_src = [batch size, src len, hid dim] -> 인코더의 마지막 layer에서 출력된 출력값\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        '''\n",
        "        0부터 target 문장을 구성하고 있는 단어의 개수인 trg_len까지의 위치에 대한 정보가 담겨야 하기 때문에\n",
        "        하나의 tensor를 초기화한 후에, 각 문장에 대해서 반복할 수 있게 해준다.  \n",
        "        '''\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "        \n",
        "        #문장의 임베딩 벡터에 positional embedding 벡터를 더한 것을 실제 디코더의 입력으로 사용함\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #디코더 layer를 여러번 거친 후, 마지막으로 출력이 된 output값이 trg에 해당한다.  \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        #trg가 출력을 위한 linear layer를 거치도록 만들어준다. \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh2Y1LsGY8tc"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1YkJMpAY8td"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        #Masked multi-head attention을 수행한다. \n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        #인코더 레이어에서 설명한 add&norm 과 동일한 과정을 수행 \n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        '''\n",
        "        디코더 block에 입력된 단어들에 대한 query 벡터와 인코더 block에 입력된 \n",
        "        각각의 단어들에 대한 key, value 벡터를 사용해서 self-attention 수행\n",
        "        '''\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-mEzyMdY8td"
      },
      "source": [
        "### Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk5jRNzkY8td"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder #전체 인코더 아키텍쳐\n",
        "        self.decoder = decoder #전체 디코더 아키텍쳐\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src 문장의 <pad> 토큰에 대하여 마스크(mask)값을 0으로 설정\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    # 타켓 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록)만들기 위해 마스크를 사용함\n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        #순서대로 앞쪽에 있는 단어들만 볼수 있도록 만들기 위해서 마스크 행렬 생성 \n",
        "        '''(마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "        '''\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask #element-wise 로 AND 연산 수행 \n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        '''\n",
        "        결과적으로 두 마스크(pad_mask 와 sub_mask)의 곱이 1이 되는 위치에 대해서만\n",
        "        가중치를 가질수 있도록 만들어서 mask 값을 생성함 \n",
        "        '''\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "\n",
        "        #src와 trg이 들어왔을 때, mask를 각각 생성한다. \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        #인코더에 소스 문장을 입력함\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #디코더는 인코더의 출력값인 enc_src와 타켓문장 trg를 통해 학습하며, 마지막으로 나온 output이 바로 번역 결과에 해당함 \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbSLXtBxY8td"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpFvA3siY8td"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab) #SRC 코퍼스에 포함된 토큰의 개수\n",
        "OUTPUT_DIM = len(TRG.vocab) #TRG 코퍼스에 포함된 토큰의 개수 \n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "#인코더와 디코더 객체 선언\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1svSXGY8td"
      },
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "#transformer 객체 선언 \n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr8GsR3yY8td",
        "outputId": "7018b52a-ec36-4ed4-e300-cf30a468eacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 9,038,341 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N7rA4nMY8te"
      },
      "outputs": [],
      "source": [
        "# 모델 가중치 파라미터 초기화 \n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6fd8I_IY8te"
      },
      "outputs": [],
      "source": [
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KFJG4qtY8te"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "#adam optimizer로 학습함 \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD7r6tuHY8te"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Loss function으로는 CrossEntropyLoss를 사용함 \n",
        "뒷 부분의 패딩(padding)에 대해서는 값 무시 즉 <pad> token에 대해서는 loss 계산을 하지 않도록 조건을 부여함 \n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc4dvrrsY8tf"
      },
      "outputs": [],
      "source": [
        "#모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #출력 단어의 마지막 인덱스<eos>는 제외\n",
        "        #입력을 할 떄는 <sos>부터 시작하도록 처리 \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        #출력 단어의 인덱스 0<sos>는 제외 \n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "\n",
        "        #모델의 출력 결과와 타겟 문장을 비교하여 손실 계산    \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        #전체 손실값 계산 \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJZinpJMY8tf"
      },
      "source": [
        "The evaluation loop is the same as the training loop, just without the gradient calculations and parameter updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFjdCJ0BY8tf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQp5zcGzY8tf"
      },
      "source": [
        "We then define a small function that we can use to tell us how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcRNmfTEY8tf"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDghg6kbY8tg"
      },
      "source": [
        "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImdWoLu7Y8tg",
        "outputId": "3af7f17f-2fa4-4c33-abc3-6bd5c0488955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 14m 33s\n",
            "\tTrain Loss: 4.128 | Train PPL:  62.068\n",
            "\t Val. Loss: 3.029 |  Val. PPL:  20.681\n",
            "Epoch: 02 | Time: 13m 35s\n",
            "\tTrain Loss: 2.816 | Train PPL:  16.706\n",
            "\t Val. Loss: 2.309 |  Val. PPL:  10.062\n",
            "Epoch: 03 | Time: 14m 3s\n",
            "\tTrain Loss: 2.234 | Train PPL:   9.335\n",
            "\t Val. Loss: 1.987 |  Val. PPL:   7.291\n",
            "Epoch: 04 | Time: 12m 16s\n",
            "\tTrain Loss: 1.884 | Train PPL:   6.577\n",
            "\t Val. Loss: 1.807 |  Val. PPL:   6.091\n",
            "Epoch: 05 | Time: 12m 18s\n",
            "\tTrain Loss: 1.636 | Train PPL:   5.136\n",
            "\t Val. Loss: 1.722 |  Val. PPL:   5.594\n",
            "Epoch: 06 | Time: 13m 1s\n",
            "\tTrain Loss: 1.449 | Train PPL:   4.261\n",
            "\t Val. Loss: 1.667 |  Val. PPL:   5.296\n",
            "Epoch: 07 | Time: 13m 27s\n",
            "\tTrain Loss: 1.296 | Train PPL:   3.656\n",
            "\t Val. Loss: 1.648 |  Val. PPL:   5.197\n",
            "Epoch: 08 | Time: 12m 56s\n",
            "\tTrain Loss: 1.166 | Train PPL:   3.209\n",
            "\t Val. Loss: 1.626 |  Val. PPL:   5.083\n",
            "Epoch: 09 | Time: 12m 1s\n",
            "\tTrain Loss: 1.061 | Train PPL:   2.888\n",
            "\t Val. Loss: 1.632 |  Val. PPL:   5.112\n",
            "Epoch: 10 | Time: 12m 37s\n",
            "\tTrain Loss: 0.967 | Train PPL:   2.631\n",
            "\t Val. Loss: 1.647 |  Val. PPL:   5.190\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    #validation loss 가 더 감소하는 경우에만 모델 파라미터를 기록하도록 만듬 \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlePpNzY8tg"
      },
      "source": [
        "We load our \"best\" parameters and manage to achieve a better test perplexity than all previous models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdgZXjDZY8tg"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr_IpnM1Y8tg"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now we can can translations from our model with the `translate_sentence` function below.\n",
        "\n",
        "The steps taken are:\n",
        "- tokenize the source sentence if it has not been tokenized (is a string)\n",
        "- append the `<sos>` and `<eos>` tokens\n",
        "- numericalize the source sentence\n",
        "- convert it to a tensor and add a batch dimension\n",
        "- create the source sentence mask\n",
        "- feed the source sentence and mask into the encoder\n",
        "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
        "- while we have not hit a maximum length\n",
        "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
        "  - create a target sentence mask\n",
        "  - place the current output, encoder output and both masks into the decoder\n",
        "  - get next output token prediction from decoder along with attention\n",
        "  - add prediction to current output sentence prediction\n",
        "  - break if the prediction was an `<eos>` token\n",
        "- convert the output sentence from indexes to tokens\n",
        "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nofUROivY8tg"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval() #평가 모드 \n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)] #문장(sentence)가 입력되었을 때, 토큰화 진행 \n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기 \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqE3wfSHY8th"
      },
      "source": [
        "We'll now define a function that displays the attention over the source sentence for each step of the decoding. As this model has 8 heads our model we can view the attention for each of the heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywOD6q6_Y8th"
      },
      "outputs": [],
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apBSTVHiY8th"
      },
      "source": [
        "First, we'll get an example from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZ9kG39Y8th"
      },
      "outputs": [],
      "source": [
        "example_idx = 8\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['src']\n",
        "trg = vars(train_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArvnFqCHY8th"
      },
      "source": [
        "Our translation looks pretty good, although our model changes *is walking by* to *walks by*. The meaning is still the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBpfUJCaY8th"
      },
      "outputs": [],
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjoBpD_nY8th"
      },
      "source": [
        "We can see the attention from each head below. Each is certainly different, but it's difficult (perhaps impossible) to reason about what head has actually learned to pay attention to. Some heads pay full attention to \"eine\" when translating \"a\", some don't at all, and some do a little. They all seem to follow the similar \"downward staircase\" pattern and the attention when outputting the last two tokens is equally spread over the final two tokens in the input sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfNAiFWDY8ti"
      },
      "outputs": [],
      "source": [
        "display_attention(src, translation, attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfA8c-w_Y8ti"
      },
      "source": [
        "Next, let's get an example the model has not been trained on from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr08-lYNY8ti"
      },
      "outputs": [],
      "source": [
        "example_idx = 6\n",
        "\n",
        "src = vars(valid_data.examples[example_idx])['src']\n",
        "trg = vars(valid_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIPOKDiY8ti"
      },
      "source": [
        "The model translates it by switching *is running* to just *runs*, but it is an acceptable swap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02U0s0NCY8ti"
      },
      "outputs": [],
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxBnRisHY8ti"
      },
      "source": [
        "Again, some heads pay full attention to \"ein\" whilst some pay no attention to it. Again, most of the heads seem to spread their attention over both the period and `<eos>` tokens in the source sentence when outputting the period and `<eos>` sentence in the predicted target sentence, though some seem to pay attention to tokens from near the start of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Zo7hhTY8ti"
      },
      "outputs": [],
      "source": [
        "display_attention(src, translation, attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIjaPrcbY8ti"
      },
      "source": [
        "Finally, we'll look at an example from the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJKaPwiQY8tj"
      },
      "outputs": [],
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['src']\n",
        "trg = vars(test_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKELKMb_Y8tj"
      },
      "source": [
        "A perfect translation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpZ4fUDoY8tj"
      },
      "outputs": [],
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFvFKIzjY8tj"
      },
      "outputs": [],
      "source": [
        "display_attention(src, translation, attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxxQi6uY8tj"
      },
      "source": [
        "## BLEU\n",
        "\n",
        "Finally we calculate the BLEU score for the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEXbGJLxY8tj"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irQ5Uf65Y8tj"
      },
      "source": [
        "We get a BLEU score of 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model. All this whilst having the least amount of parameters and the fastest training time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPK7I8GKY8tj"
      },
      "outputs": [],
      "source": [
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrSqSv-Y8tj"
      },
      "source": [
        "Congratulations for finishing these tutorials! I hope you've found them useful.\n",
        "\n",
        "If you find any mistakes or want to ask any questions about any of the code or explanations used, feel free to submit a GitHub issue and I will try to correct it ASAP.\n",
        "\n",
        "## Appendix\n",
        "\n",
        "The `calculate_bleu` function above is unoptimized. Below is a significantly faster, vectorized version of it that should be used if needed. Credit for the implementation goes to [@azadyasar](https://github.com/azadyasar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpvxEYyWY8tk"
      },
      "outputs": [],
      "source": [
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "\n",
        "    return pred_sentences, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzpLdjRTY8tk"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eURZTXr1PCAd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
