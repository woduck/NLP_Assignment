{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMHKYqe2pWHe"
      },
      "source": [
        "# Fine Tuning\n",
        "\n",
        "- 기존 모델의 학습 시간이 GPU를 사용함에도 매우 오래걸려 라이브러리를 이용해 재구성했습니다.\n",
        "\n",
        "- 데이터는 CoLa data set을 사용했고, 이에 대한 적절한 작업을 수행하기 위해, Classification을 수행하는 XLNet을 가져왔습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM2jfS7Fvvle"
      },
      "source": [
        "# 필요한 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.3.0\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "VeBL7maHHYQD",
        "outputId": "cc9c002f-ba1f-4a90-fc59-27b37b89553b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.3.0\n",
            "  Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 20.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (3.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (0.1.97)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.3.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.3.0) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6190f3e64938e8312ff2700abdf8b544501bca15b6877dde186c3e21e9b8b27b\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.25.1\n",
            "    Uninstalling transformers-4.25.1:\n",
            "      Successfully uninstalled transformers-4.25.1\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oXXH5RxixdvQ"
      },
      "outputs": [],
      "source": [
        "from transformers import *\n",
        "import torch\n",
        "from transformers import XLNetForSequenceClassification\n",
        "from transformers import XLNetTokenizer, XLNetModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91GadICcvzU4"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq \"/content/cola_public_1.1.zip\" -d \"/content/cola_public_1.1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsWehc4OLM3Y",
        "outputId": "adaebd4e-8f81-4703-aba4-ab2ab1e1a1f7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/cola_public_1.1/cola_public/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/tokenized/in_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/tokenized/in_domain_train.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/tokenized/out_of_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/raw/in_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/raw/in_domain_train.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/cola_public_1.1/cola_public/raw/out_of_domain_dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Az_i7I_cxd2v",
        "outputId": "ee9febb9-c2da-4e36-ab43-a19cfd7b02be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentence_source  label label_notes  \\\n",
              "0            gj04      1         NaN   \n",
              "1            gj04      1         NaN   \n",
              "2            gj04      1         NaN   \n",
              "\n",
              "                                            sentence  \n",
              "0  Our friends won't buy this analysis, let alone...  \n",
              "1  One more pseudo generalization and I'm giving up.  \n",
              "2   One more pseudo generalization or I'm giving up.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f15147d-460e-4551-904a-a014334804ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our friends won't buy this analysis, let alone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One more pseudo generalization and I'm giving up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One more pseudo generalization or I'm giving up.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f15147d-460e-4551-904a-a014334804ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f15147d-460e-4551-904a-a014334804ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f15147d-460e-4551-904a-a014334804ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/cola_public_1.1/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MbhkNNG1xd6H"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osExIuSUCX_K",
        "outputId": "25839461-9973-4af9-e0a0-0f4b91663be5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Inzty8nPr97-",
        "outputId": "888896d7-19a4-4c25-9d7e-444db2a7e586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0  Our friends won't buy this analysis, let alone...      1\n",
              "1  One more pseudo generalization and I'm giving up.      1\n",
              "2   One more pseudo generalization or I'm giving up.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe103767-b76b-4dcf-a275-1959f1489e83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our friends won't buy this analysis, let alone...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One more pseudo generalization and I'm giving up.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One more pseudo generalization or I'm giving up.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe103767-b76b-4dcf-a275-1959f1489e83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe103767-b76b-4dcf-a275-1959f1489e83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe103767-b76b-4dcf-a275-1959f1489e83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "df=df[['sentence','label']]\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GStfaccur95N"
      },
      "outputs": [],
      "source": [
        "sentences  = []\n",
        "for sentence in df['sentence']:\n",
        "  sentence = sentence+\"[SEP] [CLS]\"\n",
        "  sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7bmHxXmBR4-V"
      },
      "outputs": [],
      "source": [
        "# questions_2  = []\n",
        "# for question in df['question2']:\n",
        "#   question = question+\"[SEP] [CLS]\"\n",
        "#   questions_2.append(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wSYhpGnGSDUp"
      },
      "outputs": [],
      "source": [
        "# questions = []\n",
        "# questions = questions_1 + questions_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YRWv9tEfr92R",
        "outputId": "5dfd82ae-cd8b-4d2f-be47-b79c7d223a89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-21522316c57e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlnet-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlnet-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "tokenized_text = [tokenizer.tokenize(sent) for sent in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph4gAYWKr9we"
      },
      "outputs": [],
      "source": [
        "ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kaf6ZTsr9tq"
      },
      "outputs": [],
      "source": [
        "labels = df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebkd5dIvr9qn",
        "outputId": "565082dd-19ad-47bb-b9da-e11b2fc90a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n"
          ]
        }
      ],
      "source": [
        "max_ = len(ids[0])\n",
        "for i in ids:\n",
        "  max_=max(max_,len(i))\n",
        "print(max_)\n",
        "MAX_LEN = max_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTjt6IFasyEN"
      },
      "outputs": [],
      "source": [
        "input_ids2 = pad_sequences(ids,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erJhbq5ksx-s"
      },
      "outputs": [],
      "source": [
        "xtrain,xtest,ytrain,ytest = train_test_split(input_ids2,labels,test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ2_HafWr9ms"
      },
      "outputs": [],
      "source": [
        "Xtrain = torch.tensor(xtrain)\n",
        "Ytrain = torch.tensor(ytrain)\n",
        "Xtest = torch.tensor(xtest)\n",
        "Ytest = torch.tensor(ytest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCJoGRsyt7va"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "no_train = 0\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRNHPBmpuDlk"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(Xtrain,Ytrain)\n",
        "test_data = TensorDataset(Xtest,Ytest)\n",
        "loader = DataLoader(train_data,batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Aez_dZd3uDcn",
        "outputId": "c18ca68d-9d84-4dba-9a94-7c0004372917"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at C:\\Users\\woduc/.cache\\huggingface\\hub\\models--xlnet-base-cased\\snapshots\\593a21e8b79948a7f952811aa44f37d76e23d586\\config.json\n",
            "Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at C:\\Users\\woduc/.cache\\huggingface\\hub\\models--xlnet-base-cased\\snapshots\\593a21e8b79948a7f952811aa44f37d76e23d586\\pytorch_model.bin\n",
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 3.37 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [37], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m XLNetForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mxlnet-base-cased\u001b[39m\u001b[39m\"\u001b[39m,num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m# xlnet-base(large)-cased\u001b[39;00m\n\u001b[0;32m      8\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m----> 9\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     10\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:689\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    673\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \n\u001b[0;32m    675\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:689\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    673\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \n\u001b[0;32m    675\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 3.37 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
        "  correct=0\n",
        "  for i in range(0,len(labels)):\n",
        "    if(preds[i]==labels[i]):\n",
        "      correct+=1\n",
        "  return (correct/len(labels))*100\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=2) # xlnet-base(large)-cased\n",
        "torch.cuda.empty_cache()\n",
        "model.cuda()\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5) # initial learning rate 0.00002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vruagmjgt7Oz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27GQXXuBxPS6"
      },
      "source": [
        "# 모델 학습\n",
        "\n",
        "- 논문에서는 시그모이드 함수를 loss로 정의했다고 나와있지만, 이를 cross entropy를 사용해보니 CoLa데이터셋에 한하여 더 좋은 성능을 보이는 것을 확인할 수 있습니다. 물론, 시간이 지나며 혹은 라이브러리에 들어간 함수의 코드가 달라졌을 가능성을 감안해야 할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNourTzpuZB_",
        "outputId": "9018291b-6f1c-4038-904e-70ef0f8413ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------\n",
            "Epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/29 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 162.00 MiB (GPU 0; 4.00 GiB total capacity; 3.19 GiB already allocated; 0 bytes free; 3.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [23], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m labels1\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     14\u001b[0m labels2 \u001b[39m=\u001b[39m labels1\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,labels2)\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1548\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1540\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1545\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1546\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1548\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m   1549\u001b[0m     input_ids,\n\u001b[0;32m   1550\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1551\u001b[0m     mems\u001b[39m=\u001b[39mmems,\n\u001b[0;32m   1552\u001b[0m     perm_mask\u001b[39m=\u001b[39mperm_mask,\n\u001b[0;32m   1553\u001b[0m     target_mapping\u001b[39m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1554\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1555\u001b[0m     input_mask\u001b[39m=\u001b[39minput_mask,\n\u001b[0;32m   1556\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1557\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1558\u001b[0m     use_mems\u001b[39m=\u001b[39muse_mems,\n\u001b[0;32m   1559\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1560\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1561\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1562\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1563\u001b[0m )\n\u001b[0;32m   1564\u001b[0m output \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1566\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequence_summary(output)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1241\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1239\u001b[0m     hidden_states\u001b[39m.\u001b[39mappend((output_h, output_g) \u001b[39mif\u001b[39;00m output_g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m output_h)\n\u001b[1;32m-> 1241\u001b[0m outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1242\u001b[0m     output_h,\n\u001b[0;32m   1243\u001b[0m     output_g,\n\u001b[0;32m   1244\u001b[0m     attn_mask_h\u001b[39m=\u001b[39;49mnon_tgt_mask,\n\u001b[0;32m   1245\u001b[0m     attn_mask_g\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m   1246\u001b[0m     r\u001b[39m=\u001b[39;49mpos_emb,\n\u001b[0;32m   1247\u001b[0m     seg_mat\u001b[39m=\u001b[39;49mseg_mat,\n\u001b[0;32m   1248\u001b[0m     mems\u001b[39m=\u001b[39;49mmems[i],\n\u001b[0;32m   1249\u001b[0m     target_mapping\u001b[39m=\u001b[39;49mtarget_mapping,\n\u001b[0;32m   1250\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m   1251\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1252\u001b[0m )\n\u001b[0;32m   1253\u001b[0m output_h, output_g \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m   1254\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:527\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m output_g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m     output_g \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff_chunk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim, output_g\n\u001b[0;32m    526\u001b[0m     )\n\u001b[1;32m--> 527\u001b[0m output_h \u001b[39m=\u001b[39m apply_chunking_to_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, output_h)\n\u001b[0;32m    529\u001b[0m outputs \u001b[39m=\u001b[39m (output_h, output_g) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Add again attentions if there are there\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:533\u001b[0m, in \u001b[0;36mXLNetLayer.ff_chunk\u001b[1;34m(self, output_x)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mff_chunk\u001b[39m(\u001b[39mself\u001b[39m, output_x):\n\u001b[1;32m--> 533\u001b[0m     output_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(output_x)\n\u001b[0;32m    534\u001b[0m     \u001b[39mreturn\u001b[39;00m output_x\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:480\u001b[0m, in \u001b[0;36mXLNetFeedForward.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    478\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_1(output)\n\u001b[0;32m    479\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_function(output)\n\u001b[1;32m--> 480\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(output)\n\u001b[0;32m    481\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_2(output)\n\u001b[0;32m    482\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(output)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[1;32mc:\\Users\\woduc\\Anaconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 0; 4.00 GiB total capacity; 3.19 GiB already allocated; 0 bytes free; 3.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(epochs):\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(\"Epoch:\",epoch+1)\n",
        "  model.train()\n",
        "  loss1 = []\n",
        "  train_loss = []\n",
        "  l = []\n",
        "  for inputs,labels1 in tqdm(loader) :\n",
        "    inputs.to(device)\n",
        "    labels1.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.to(device))\n",
        "    labels2 = labels1.to(device)\n",
        "    loss = criterion(outputs,labels2).to(device)\n",
        "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n",
        "    [l.append(z.item()) for z in labels1]# real labels\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss1.append(loss.item())\n",
        "    no_train += inputs.size(0)\n",
        "  print(\"Loss :\",loss.item())\n",
        "  print(\"Accuracy :\",flat_accuracy(train_loss,l))\n",
        "  print(\"-----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LRzdPn-Cp6H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "moon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "76d8510afb4a7bd73af5a1710ec4d08a252c4f32031e14ee5cbb196e47b01a3c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}